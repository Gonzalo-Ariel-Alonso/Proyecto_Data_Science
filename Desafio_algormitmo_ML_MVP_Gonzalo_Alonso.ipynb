{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d2cc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix , classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc6310",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f21b355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.604462</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0</td>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?   ROA(C) before interest and depreciation before interest   \n",
       "0             1                                           0.370594         \\\n",
       "1             1                                           0.464291          \n",
       "2             1                                           0.426071          \n",
       "3             1                                           0.399844          \n",
       "4             1                                           0.465022          \n",
       "...         ...                                                ...          \n",
       "6814          0                                           0.493687          \n",
       "6815          0                                           0.475162          \n",
       "6816          0                                           0.472725          \n",
       "6817          0                                           0.506264          \n",
       "6818          0                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax   \n",
       "0                                    0.424389  \\\n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax   \n",
       "0                                              0.405750   \\\n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Realized Sales Gross Margin   \n",
       "0                    0.601457                      0.601457  \\\n",
       "1                    0.610235                      0.610235   \n",
       "2                    0.601450                      0.601364   \n",
       "3                    0.583541                      0.583541   \n",
       "4                    0.598783                      0.598783   \n",
       "...                       ...                           ...   \n",
       "6814                 0.604455                      0.604462   \n",
       "6815                 0.598308                      0.598308   \n",
       "6816                 0.610444                      0.610213   \n",
       "6817                 0.607850                      0.607850   \n",
       "6818                 0.627409                      0.627409   \n",
       "\n",
       "       Operating Profit Rate   Pre-tax net Interest Rate   \n",
       "0                   0.998969                    0.796887  \\\n",
       "1                   0.998946                    0.797380   \n",
       "2                   0.998857                    0.796403   \n",
       "3                   0.998700                    0.796967   \n",
       "4                   0.998973                    0.797366   \n",
       "...                      ...                         ...   \n",
       "6814                0.998992                    0.797409   \n",
       "6815                0.998992                    0.797414   \n",
       "6816                0.998984                    0.797401   \n",
       "6817                0.999074                    0.797500   \n",
       "6818                0.998080                    0.801987   \n",
       "\n",
       "       After-tax net Interest Rate   \n",
       "0                         0.808809  \\\n",
       "1                         0.809301   \n",
       "2                         0.808388   \n",
       "3                         0.808966   \n",
       "4                         0.809304   \n",
       "...                            ...   \n",
       "6814                      0.809331   \n",
       "6815                      0.809327   \n",
       "6816                      0.809317   \n",
       "6817                      0.809399   \n",
       "6818                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  ...   \n",
       "0                                         0.302646  ...  \\\n",
       "1                                         0.303556  ...   \n",
       "2                                         0.302035  ...   \n",
       "3                                         0.303350  ...   \n",
       "4                                         0.303475  ...   \n",
       "...                                            ...  ...   \n",
       "6814                                      0.303510  ...   \n",
       "6815                                      0.303520  ...   \n",
       "6816                                      0.303512  ...   \n",
       "6817                                      0.303498  ...   \n",
       "6818                                      0.313415  ...   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price   \n",
       "0                        0.716845                    0.009219  \\\n",
       "1                        0.795297                    0.008323   \n",
       "2                        0.774670                    0.040003   \n",
       "3                        0.739555                    0.003252   \n",
       "4                        0.795016                    0.003878   \n",
       "...                           ...                         ...   \n",
       "6814                     0.799927                    0.000466   \n",
       "6815                     0.799748                    0.001959   \n",
       "6816                     0.797778                    0.002840   \n",
       "6817                     0.811808                    0.002837   \n",
       "6818                     0.815956                    0.000707   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales   \n",
       "0                0.622879                0.601453  \\\n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity   \n",
       "0                                0.827890              0.290202  \\\n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)   \n",
       "0                                0.026601  \\\n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag   \n",
       "0                                              0.564050                   1  \\\n",
       "1                                              0.570175                   1   \n",
       "2                                              0.563706                   1   \n",
       "3                                              0.564663                   1   \n",
       "4                                              0.575617                   1   \n",
       "...                                                 ...                 ...   \n",
       "6814                                           0.566193                   1   \n",
       "6815                                           0.566018                   1   \n",
       "6816                                           0.565158                   1   \n",
       "6817                                           0.565302                   1   \n",
       "6818                                           0.565167                   1   \n",
       "\n",
       "       Equity to Liability  \n",
       "0                 0.016469  \n",
       "1                 0.020794  \n",
       "2                 0.016474  \n",
       "3                 0.023982  \n",
       "4                 0.035490  \n",
       "...                    ...  \n",
       "6814              0.029890  \n",
       "6815              0.038284  \n",
       "6816              0.097649  \n",
       "6817              0.044009  \n",
       "6818              0.233902  \n",
       "\n",
       "[6819 rows x 96 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBP = pd.read_csv('https://media.githubusercontent.com/media/Gonzalo-Ariel-Alonso/Proyecto_Data_Science/master/Company%20Bankruptcy%20Prediction.csv')\n",
    "CBP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aff4f6",
   "metadata": {},
   "source": [
    "### Escalado de datos\n",
    "Para evitar cualquier tipo de sesgo numerico por la diferencia de dimenciones, llevamos todos los datos a la misma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cbe64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "scaler = MMS()\n",
    "CBP_scaled = scaler.fit_transform(CBP)\n",
    "CBP_scaled = pd.DataFrame(CBP_scaled, columns = list(CBP.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1ffd5",
   "metadata": {},
   "source": [
    "## Variables mas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23434be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>Debt ratio %</th>\n",
       "      <th>Current Liability to Assets</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>Net worth/Assets</th>\n",
       "      <th>Persistent EPS in the Last Four Seasons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.147308</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.792424</td>\n",
       "      <td>0.169141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.056963</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.828824</td>\n",
       "      <td>0.208944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.207516</td>\n",
       "      <td>0.098162</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.792484</td>\n",
       "      <td>0.180581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.151465</td>\n",
       "      <td>0.098715</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.193722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106509</td>\n",
       "      <td>0.110195</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.893491</td>\n",
       "      <td>0.212537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124618</td>\n",
       "      <td>0.103838</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.875382</td>\n",
       "      <td>0.216602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>0.089901</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.900747</td>\n",
       "      <td>0.216697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.961061</td>\n",
       "      <td>0.210929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086979</td>\n",
       "      <td>0.083199</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.913021</td>\n",
       "      <td>0.228326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.227758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?   Debt ratio %   Current Liability to Assets   \n",
       "0           1.0       0.207576                      0.147308  \\\n",
       "1           1.0       0.171176                      0.056963   \n",
       "2           1.0       0.207516                      0.098162   \n",
       "3           1.0       0.151465                      0.098715   \n",
       "4           1.0       0.106509                      0.110195   \n",
       "...         ...            ...                           ...   \n",
       "6814        0.0       0.124618                      0.103838   \n",
       "6815        0.0       0.099253                      0.089901   \n",
       "6816        0.0       0.038939                      0.024414   \n",
       "6817        0.0       0.086979                      0.083199   \n",
       "6818        0.0       0.014149                      0.018517   \n",
       "\n",
       "       Net Income to Total Assets   ROA(A) before interest and % after tax   \n",
       "0                        0.716845                                 0.424389  \\\n",
       "1                        0.795297                                 0.538214   \n",
       "2                        0.774670                                 0.499019   \n",
       "3                        0.739555                                 0.451265   \n",
       "4                        0.795016                                 0.538432   \n",
       "...                           ...                                      ...   \n",
       "6814                     0.799927                                 0.539468   \n",
       "6815                     0.799748                                 0.538269   \n",
       "6816                     0.797778                                 0.533744   \n",
       "6817                     0.811808                                 0.559911   \n",
       "6818                     0.815956                                 0.570105   \n",
       "\n",
       "       Net worth/Assets   Persistent EPS in the Last Four Seasons  \n",
       "0              0.792424                                  0.169141  \n",
       "1              0.828824                                  0.208944  \n",
       "2              0.792484                                  0.180581  \n",
       "3              0.848535                                  0.193722  \n",
       "4              0.893491                                  0.212537  \n",
       "...                 ...                                       ...  \n",
       "6814           0.875382                                  0.216602  \n",
       "6815           0.900747                                  0.216697  \n",
       "6816           0.961061                                  0.210929  \n",
       "6817           0.913021                                  0.228326  \n",
       "6818           0.985851                                  0.227758  \n",
       "\n",
       "[6819 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBP_scaled_filtered = CBP_scaled[['Bankrupt?',' Debt ratio %',' Current Liability to Assets',' Net Income to Total Assets',' ROA(A) before interest and % after tax',' Net worth/Assets',' Persistent EPS in the Last Four Seasons']]\n",
    "CBP_scaled_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac3fd4",
   "metadata": {},
   "source": [
    "Ya tenemos las 6 variables mas influyentes sobre la bancarrota en nuestro dataset, ahora separemos la variable independiente (Bankrupt?) de las dependientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c624be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CBP_scaled_filtered.iloc[:,1:]\n",
    "y = CBP_scaled_filtered.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f6c30",
   "metadata": {},
   "source": [
    "## Oversampling / Balanceo de clases\n",
    "Tenemos dos tipos de clases, companias en bancarrota y operativas, no obstante la muestra de companias operativas en mucho mayor a la de bancarrota, para evistar cualquier tipo de sesgo de nuestro modelo utilizaremos el metodo de oversampling con la libreria de imblearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f60e56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 6599, 1.0: 220})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Muestra desbalanceada\n",
    "count = Counter(y)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc8bf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 6599, 0.0: 6599})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE()\n",
    "x_os , y_os = os.fit_resample(x,y)\n",
    "\n",
    "# Muestra balanceada\n",
    "count = Counter(y_os)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a00218",
   "metadata": {},
   "source": [
    "## Regrecion Logistica\n",
    "Nuestro dataset nos precenta un problema de clacificaion binaria, asi que para entrenar un modelo de machine learning podemos utilizar el algoritmo de regresion logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64dd1246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separacion de datos de prueba y entrenamiento\n",
    "X_train, X_test , y_train , y_test = train_test_split(x_os , y_os ,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a629487",
   "metadata": {},
   "source": [
    "### Hiperparametros\n",
    "Utilizando la funcion grid search de sklearn podemos probrar diferentes parametros en nuestro modelo para encotrar cuales son los que dan mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de658b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Hiperparametros\n",
    "params = {\n",
    "    'penalty':['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.5,0.75,1.0,1.5,1.75,2.0,2.25],\n",
    "    'intercept_scaling': [0.5,0.75,1.0,1.75,1.5],\n",
    "    'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky','sag', 'saga'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "LR = LogisticRegression()\n",
    "\n",
    "grid_logistic_regression = GridSearchCV(estimator = LR,\n",
    "                                        param_grid = params,\n",
    "                                        scoring = 'neg_mean_absolute_error',\n",
    "                                        cv = 5,\n",
    "                                        verbose = 1,\n",
    "                                        n_jobs = -1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9562cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 630 candidates, totalling 3150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1750 fits failed out of a total of 3150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "175 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "                   ^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "                         ~~^~~~~~~~~~\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan -0.13345369         nan         nan         nan -0.13155948\n",
      " -0.13923177 -0.14766146 -0.13923177 -0.13923177 -0.13923182 -0.13923177\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13345373         nan         nan         nan -0.13155948\n",
      " -0.13923177 -0.14680883 -0.13923177 -0.13923177 -0.13932652 -0.13923177\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.1331696          nan         nan         nan -0.13155948\n",
      " -0.13923177 -0.1449144  -0.13923177 -0.13923177 -0.13923177 -0.13923177\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13174892         nan         nan         nan -0.13155948\n",
      " -0.13923177 -0.14283071 -0.13923177 -0.13923177 -0.13932652 -0.13923177\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.1323171          nan         nan         nan -0.13155948\n",
      " -0.13923177 -0.14349395 -0.13923177 -0.13923177 -0.13923177 -0.13923177\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13250667         nan         nan         nan -0.13127521\n",
      " -0.13894728 -0.1456722  -0.13894728 -0.13894728 -0.13904202 -0.13894728\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13080168         nan         nan         nan -0.13127521\n",
      " -0.13894728 -0.14377799 -0.13894728 -0.13894728 -0.13885258 -0.13894728\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13099112         nan         nan         nan -0.13127521\n",
      " -0.13894728 -0.14226244 -0.13894728 -0.13894728 -0.13894728 -0.13894728\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13089647         nan         nan         nan -0.13127521\n",
      " -0.13894728 -0.13875816 -0.13894728 -0.13894728 -0.13904198 -0.13894728\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13089642         nan         nan         nan -0.13127521\n",
      " -0.13894728 -0.13942108 -0.13894728 -0.13894728 -0.13885258 -0.13894728\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13108591         nan         nan         nan -0.13051759\n",
      " -0.13818952 -0.14283089 -0.13818952 -0.13818952 -0.13818952 -0.13818952\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13061229         nan         nan         nan -0.13051759\n",
      " -0.13818952 -0.14103129 -0.13818952 -0.13818952 -0.13828422 -0.13818952\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13032815         nan         nan         nan -0.13051759\n",
      " -0.13818952 -0.13856858 -0.13818952 -0.13818952 -0.13818952 -0.13818952\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13061233         nan         nan         nan -0.13051759\n",
      " -0.13818952 -0.13875789 -0.13818952 -0.13818952 -0.13828422 -0.13818952\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13042285         nan         nan         nan -0.13051759\n",
      " -0.13818952 -0.13885258 -0.13818952 -0.13818952 -0.13828422 -0.13818952\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13061224         nan         nan         nan -0.12900217\n",
      " -0.13724229 -0.13894724 -0.13724229 -0.13724229 -0.13724229 -0.13724229\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13023332         nan         nan         nan -0.12900217\n",
      " -0.13724229 -0.13894719 -0.13724229 -0.13724229 -0.13724229 -0.13724229\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13004388         nan         nan         nan -0.12900217\n",
      " -0.13724229 -0.13818952 -0.13724229 -0.13724229 -0.13733698 -0.13724229\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12985449         nan         nan         nan -0.12900217\n",
      " -0.13724229 -0.13752656 -0.13724229 -0.13724229 -0.13724229 -0.13724229\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13013867         nan         nan         nan -0.12900217\n",
      " -0.13724229 -0.13790539 -0.13724229 -0.13724229 -0.13724229 -0.13724229\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13061224         nan         nan         nan -0.1284339\n",
      " -0.13676858 -0.13866301 -0.13676858 -0.13676858 -0.13676858 -0.13676858\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12957026         nan         nan         nan -0.1284339\n",
      " -0.13676858 -0.13828418 -0.13676858 -0.13676858 -0.13676858 -0.13676858\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12957035         nan         nan         nan -0.1284339\n",
      " -0.13676858 -0.13743172 -0.13676858 -0.13676858 -0.13676858 -0.13676858\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12919156         nan         nan         nan -0.1284339\n",
      " -0.13676858 -0.13705289 -0.13676858 -0.13676858 -0.13676858 -0.13676858\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12919156         nan         nan         nan -0.1284339\n",
      " -0.13676858 -0.13762125 -0.13676858 -0.13676858 -0.13676858 -0.13676858\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.13004397         nan         nan         nan -0.12814976\n",
      " -0.13657909 -0.13771582 -0.13657909 -0.13657909 -0.13657909 -0.13657909\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.1294757          nan         nan         nan -0.12814976\n",
      " -0.13657909 -0.13781047 -0.13657909 -0.13657909 -0.13657909 -0.13657909\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12900212         nan         nan         nan -0.12814976\n",
      " -0.13657909 -0.13762112 -0.13657909 -0.13657909 -0.13657909 -0.13657909\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12890747         nan         nan         nan -0.12814976\n",
      " -0.13657909 -0.13686336 -0.13657909 -0.13657909 -0.13657909 -0.13657909\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12881278         nan         nan         nan -0.12814976\n",
      " -0.13657909 -0.13705276 -0.13657909 -0.13657909 -0.13657909 -0.13657909\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12900212         nan         nan         nan -0.12824446\n",
      " -0.1364844  -0.13762107 -0.1364844  -0.1364844  -0.1364844  -0.1364844\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12909691         nan         nan         nan -0.12824446\n",
      " -0.1364844  -0.13752638 -0.1364844  -0.1364844  -0.1364844  -0.1364844\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12871799         nan         nan         nan -0.12824446\n",
      " -0.1364844  -0.1371475  -0.1364844  -0.1364844  -0.1364844  -0.1364844\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12814976         nan         nan         nan -0.12824446\n",
      " -0.1364844  -0.13638965 -0.1364844  -0.1364844  -0.1364844  -0.1364844\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.12824446         nan         nan         nan -0.12824446\n",
      " -0.1364844  -0.13657914 -0.1364844  -0.1364844  -0.1364844  -0.1364844\n",
      "         nan         nan         nan         nan         nan         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.39 s\n",
      "Wall time: 2min 26s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.5, 0.75, 1.0, 1.5, 1.75, 2.0, 2.25],\n",
       "                         &#x27;intercept_scaling&#x27;: [0.5, 0.75, 1.0, 1.75, 1.5],\n",
       "                         &#x27;max_iter&#x27;: [1000],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [0.5, 0.75, 1.0, 1.5, 1.75, 2.0, 2.25],\n",
       "                         &#x27;intercept_scaling&#x27;: [0.5, 0.75, 1.0, 1.75, 1.5],\n",
       "                         &#x27;max_iter&#x27;: [1000],\n",
       "                         &#x27;penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;],\n",
       "                         &#x27;solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                    &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [0.5, 0.75, 1.0, 1.5, 1.75, 2.0, 2.25],\n",
       "                         'intercept_scaling': [0.5, 0.75, 1.0, 1.75, 1.5],\n",
       "                         'max_iter': [1000],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet'],\n",
       "                         'solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
       "                                    'newton-cholesky', 'sag', 'saga']},\n",
       "             scoring='neg_mean_absolute_error', verbose=1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_logistic_regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff916f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.0,\n",
       " 'intercept_scaling': 0.5,\n",
       " 'max_iter': 1000,\n",
       " 'penalty': 'l1',\n",
       " 'solver': 'saga'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_logistic_regression.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380152b2",
   "metadata": {},
   "source": [
    "### Entrenamiento refinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e94829c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR_best_params =  LogisticRegression(C= 2.0,\n",
    " intercept_scaling= 0.5,\n",
    " max_iter= 1000,\n",
    " penalty= 'l1',\n",
    " solver= 'saga')\n",
    "LR_best_params .fit(X_train,y_train)\n",
    "\n",
    "# Prediccion 2\n",
    "y_pred_2 = LR_best_params.predict(X_test)\n",
    "\n",
    "# Matriz de confucion\n",
    "cm_2 = confusion_matrix(y_test, y_pred_2)\n",
    "sns.heatmap(data = cm_2, annot = True,cmap='Blues', fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82ef55e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.86      0.86      1328\n",
      "         1.0       0.86      0.87      0.86      1312\n",
      "\n",
      "    accuracy                           0.86      2640\n",
      "   macro avg       0.86      0.86      0.86      2640\n",
      "weighted avg       0.86      0.86      0.86      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c845c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
